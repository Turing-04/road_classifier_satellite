Please append to this file the current accuracy you have for a given model (Unet, CNN, Xception... ? ) with the number of epochs / hyperparams
And especially the loss at after each epoch -> To be recorded here after each training period

Essential for the report to come !


30 epochs, U-net -> 0.895 F1 score 0.933, training dataset with 1000 samples (from sratch weights)


60 epochs, U-net -> 0.895 F1 score 0.945, training dataset with 1100 samples (antoine, only rotations / flipping but with repeting dataset) (from scratch weights)


80 epochs, Basic U-net -> 0.875, F1 score:0.934, Marin's data augmentation (900 samples):
losses=[0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418, 0.1534139349228806, 0.15346083755294482, 0.15274518079227872, 0.14909644103712505, 0.1469741752743721, 0.148070676939355, 0.16489513190256225, 0.1486604721016354, 0.14407496187422011, 0.14418292934695878, 0.14439637523558405, 0.17220824946959815, 0.14562142946653897, 0.1422411410841677, 0.14169667661190033, 0.16542590166131654, 0.14384175742665928, 0.14193710757626427, 0.14081118105186358, 0.1414958649377028, 0.14115476056933404, 0.14301788061857224, 0.14264949204193222, 0.14213492467999458, 0.1540983316467868, 0.1432336950302124, 0.13879902644289865, 0.13845295078224606, 0.13918791299064953, 0.14067982910407914, 0.1399626374575827, 0.13998906296160485, 0.1409540914495786, 0.14431852747996649, 0.14450248209966554, 0.1390339373714394, 0.15481124737196497, 0.138491816404793, 0.13514803055259916, 0.13461955037381915]

same but only 40 epoch - 0.885, F1=0.940: 
losses = [0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418]

same with 55 epoch - acc=0.889 F1=0.941:
losses=[0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418, 0.1534139349228806, 0.15346083755294482, 0.15274518079227872, 0.14909644103712505, 0.1469741752743721, 0.148070676939355, 0.16489513190256225, 0.1486604721016354, 0.14407496187422011, 0.14418292934695878, 0.14439637523558405, 0.17220824946959815, 0.14562142946653897, 0.1422411410841677, 0.14169667661190033]

50 epochs, cnn16, F1:0.560,	acc:0.781
[1.4612571475240919, 1.3821242952346802, 1.3276965747939216, 1.278738153245714, 1.2363285130924648, 1.2042076455222235, 1.1807094981935289, 1.1563590003384485, 1.1370153286721971, 1.1222055651081932, 1.1078563059700859, 1.0966064258416495, 1.088292614751392, 1.0806414274374645, 1.0752597469753689, 1.0678815428415935, 1.0633759187327492, 1.0595504997836218, 1.058351985613505, 1.0532632244957818, 1.0512659102016024, 1.0518066730764177, 1.0493853500154284, 1.0458776108423868, 1.0410099233521355, 1.0451514707671272, 1.0445100017388662, 1.0404670935206943, 1.0436071097850799, 1.0361200388272602, 1.0333451669745974, 1.0404689719941882, 1.0316878305541144, 1.0356717885865105, 1.0329800502459208, 1.031337748368581, 1.0324597701761458, 1.0280844516224332, 1.0336266794469622, 1.0294600987434388, 1.029387230740653, 1.0293171841568416, 1.0296000085936652, 1.0273783842722575, 1.0276922982268863, 1.024765402475993, 1.0228810180558099, 1.025966581636005, 1.0279537264506022, 1.0236940484576755]

50 epochs, cnn4, F1:0.591,	acc:0.803
[1.3029563607109917, 1.2517039288414848, 1.21849035554462, 1.194526960849762, 1.1709564418262906, 1.1538910694917044, 1.1273357105255126, 1.0946927369965447, 1.0747257607513003, 1.0631217186980777, 1.050767591661877, 1.046948635313246, 1.0409223857190875, 1.036800389819675, 1.0363096311357287, 1.030641153521008, 1.0288906863000657, 1.0258088764879438, 1.0260296408335368, 1.0237021402517954, 1.025619367758433, 1.0226485231187608, 1.0194274469216664, 1.0191140210628509, 1.0204369299941594, 1.0178564993540447, 1.0141026037269167, 1.0153743267059325, 1.0119534611701966, 1.0116905433601804, 1.0088933424154918, 1.0094050199455684, 1.0118589198589325, 1.0069289984967973, 1.0142938519848717, 1.00818874835968, 1.0100191685888502, 1.0124075763755374, 1.0090126225683425, 1.006687547100915, 1.0103006439738804, 1.0066130636798012, 1.0065248771508535, 1.0061466281943852, 1.004367254310184, 1.005252998669942, 1.005622718334198, 1.0039466410213047, 1.0063876582516564, 1.0049560157457988]


50 epochs, cnn2, F1:0.624, acc: 0.816
[1.3286846963564556, 1.2736767212549844, 1.235722230805291, 1.1835708226097954, 1.1399078626102872, 1.1071506477726831, 1.0845456087589265, 1.065623428026835, 1.0521723373730978, 1.0383191647794512, 1.027482761674457, 1.0186108214325376, 1.0151081828276316, 1.0119796416494582, 1.0098648635546366, 1.0043046613534292, 0.9981239528126187, 0.9975627969370948, 0.9968840078512827, 1.0001859533786774, 0.9960802866352929, 0.9905095389154223, 0.9883073867691887, 0.9873217199908363, 0.9905941424104903, 0.9918426118956671, 0.9863332410653433, 0.9891126884354485, 0.9840185358789232, 0.9874448613325755, 0.9820541761981116, 0.979367305305269, 0.9818004773722755, 0.9790896509753333, 0.9770476207468245, 0.9811107301712036, 0.970694936381446, 0.9775416535801358, 0.9745123304261102, 0.9735032074981266, 0.9769770831531949, 0.9722391449080573, 0.97421428071128, 0.9733849537372589, 0.9711684362093608, 0.9721378178066677, 0.9711245475875007, 0.9677990047136943, 0.9733020830154419, 0.9712233134110768]

50 epochs, cnn8, F1: 0.554, acc: 0.800
[1.5586756091647678, 1.4605056929588318, 1.4021497941017151, 1.3518880359331766, 1.3100710344314574, 1.2745235747761197, 1.2417953130933974, 1.2126108479499818, 1.1859637170367772, 1.1622294678952958, 1.141787407928043, 1.1223552689287397, 1.1065204768710666, 1.0953971486621432, 1.081367579433653, 1.0664101282755534, 1.0624842802683512, 1.0508561046918232, 1.0461829348405203, 1.042060103946262, 1.0409312799241808, 1.0311749249034459, 1.0283355422814686, 1.0296556454234653, 1.035030593474706, 1.0285386796792348, 1.0279461379845938, 1.0237841737270355, 1.0231368466218314, 1.0220510600672827, 1.022900956471761, 1.0219719984796312, 1.0192062275939517, 1.0188792272408804, 1.0162661618656583, 1.01520563032892, 1.0182915759086608, 1.0139401557710435, 1.0150955737961662, 1.0159433420499167, 1.0154965586132474, 1.0165765990151299, 1.0153937281502619, 1.0114351190461053, 1.0122538846068911, 1.0124932984511057, 1.0065509924623701, 1.0092794570657941, 1.00691515300009, 1.0052200384934744]



50 epochs, unet, augmented antoine, 2400 images, Acc=0.889, F1=0.941:

50 epochs, unet, augmented 1000 images, Acc=0.896 , F1=0.945 :
same with 30 epochs : Acc=0.890 , F1=0.943
[0.812432804239043 0.515676827609539, 0.43422027152776715, 0.38944468158483503, 0.32205598801374435, 0.3143287942707539, 0.2926079306602478, 0.27735236194729807, 0.2679508848786354, 0.25783844286203383, 0.237771953612566, 0.2331615104675293, 0.22284883573651315, 0.21616415095329283, 0.23829474225640296, 0.20750801980495454, 0.20003696730732917, 0.19065733110904692, 0.1829630465656519, 0.18507903648912907, 0.1824516045451164, 0.17359677824378014, 0.17479812598228456, 0.181931421905756, 0.1660964816957712, 0.16370693074166776, 0.1693661757558584, 0.16337774297595023, 0.15596185213327407, 0.1867123684734106, 0.15863497938215732, 0.15402570262551307, 0.15305393105745316, 0.1561298328638077, 0.15339575804769992, 0.15302075025439263, 0.15208138731122017, 0.17658385829627513, 0.151592616006732, 0.16993521781265736, 0.14570591612160205, 0.14400990357995033, 0.1453236635327339, 0.14557994137704372, 0.14727802212536334, 0.15946578881144524, 0.14470911438763143, 0.1447217719554901, 0.17191597570478917, 0.15311779890954494]


50 epochs, resunet, 1000 augmented dataset size, Acc=0.893 , F1=0.944
Dataset Size:
Train: 1000 - Valid: 100

Valid loss improved from inf to 0.5519. Saving checkpoint: weights/checkpoint.pth
Epoch: 01 | Epoch Time: 0m 32s
	Train Loss: 0.772
	 Val. Loss: 0.552

Valid loss improved from 0.5519 to 0.4258. Saving checkpoint: weights/checkpoint.pth
Epoch: 02 | Epoch Time: 0m 24s
	Train Loss: 0.522
	 Val. Loss: 0.426

Valid loss improved from 0.4258 to 0.4150. Saving checkpoint: weights/checkpoint.pth
Epoch: 03 | Epoch Time: 0m 24s
	Train Loss: 0.438
	 Val. Loss: 0.415

Valid loss improved from 0.4150 to 0.3191. Saving checkpoint: weights/checkpoint.pth
Epoch: 04 | Epoch Time: 0m 24s
	Train Loss: 0.392
	 Val. Loss: 0.319

Valid loss improved from 0.3191 to 0.3150. Saving checkpoint: weights/checkpoint.pth
Epoch: 05 | Epoch Time: 0m 24s
	Train Loss: 0.351
	 Val. Loss: 0.315

Epoch: 06 | Epoch Time: 0m 24s
	Train Loss: 0.330
	 Val. Loss: 0.333

Valid loss improved from 0.3150 to 0.2704. Saving checkpoint: weights/checkpoint.pth
Epoch: 07 | Epoch Time: 0m 24s
	Train Loss: 0.318
	 Val. Loss: 0.270

Epoch: 08 | Epoch Time: 0m 24s
	Train Loss: 0.293
	 Val. Loss: 0.272

Epoch: 09 | Epoch Time: 0m 24s
	Train Loss: 0.295
	 Val. Loss: 0.314

Valid loss improved from 0.2704 to 0.2277. Saving checkpoint: weights/checkpoint.pth
Epoch: 10 | Epoch Time: 0m 24s
	Train Loss: 0.278
	 Val. Loss: 0.228

Valid loss improved from 0.2277 to 0.2242. Saving checkpoint: weights/checkpoint.pth
Epoch: 11 | Epoch Time: 0m 24s
	Train Loss: 0.253
	 Val. Loss: 0.224

Epoch: 12 | Epoch Time: 0m 24s
	Train Loss: 0.244
	 Val. Loss: 0.229

Epoch: 13 | Epoch Time: 0m 24s
	Train Loss: 0.234
	 Val. Loss: 0.228

Epoch: 14 | Epoch Time: 0m 24s
	Train Loss: 0.254
	 Val. Loss: 0.253

Valid loss improved from 0.2242 to 0.1912. Saving checkpoint: weights/checkpoint.pth
Epoch: 15 | Epoch Time: 0m 24s
	Train Loss: 0.236
	 Val. Loss: 0.191

Valid loss improved from 0.1912 to 0.1831. Saving checkpoint: weights/checkpoint.pth
Epoch: 16 | Epoch Time: 0m 24s
	Train Loss: 0.210
	 Val. Loss: 0.183

Epoch: 17 | Epoch Time: 0m 24s
	Train Loss: 0.212
	 Val. Loss: 0.191

Valid loss improved from 0.1831 to 0.1732. Saving checkpoint: weights/checkpoint.pth
Epoch: 18 | Epoch Time: 0m 24s
	Train Loss: 0.197
	 Val. Loss: 0.173

Epoch: 19 | Epoch Time: 0m 24s
	Train Loss: 0.185
	 Val. Loss: 0.196

Epoch: 20 | Epoch Time: 0m 24s
	Train Loss: 0.194
	 Val. Loss: 0.270

Epoch: 21 | Epoch Time: 0m 24s
	Train Loss: 0.200
	 Val. Loss: 0.176

Valid loss improved from 0.1732 to 0.1674. Saving checkpoint: weights/checkpoint.pth
Epoch: 22 | Epoch Time: 0m 24s
	Train Loss: 0.176
	 Val. Loss: 0.167

Valid loss improved from 0.1674 to 0.1650. Saving checkpoint: weights/checkpoint.pth
Epoch: 23 | Epoch Time: 0m 24s
	Train Loss: 0.175
	 Val. Loss: 0.165

Valid loss improved from 0.1650 to 0.1575. Saving checkpoint: weights/checkpoint.pth
Epoch: 24 | Epoch Time: 0m 24s
	Train Loss: 0.173
	 Val. Loss: 0.157

Valid loss improved from 0.1575 to 0.1570. Saving checkpoint: weights/checkpoint.pth
Epoch: 25 | Epoch Time: 0m 24s
	Train Loss: 0.167
	 Val. Loss: 0.157

Epoch: 26 | Epoch Time: 0m 24s
	Train Loss: 0.164
	 Val. Loss: 0.164

Epoch: 27 | Epoch Time: 0m 24s
	Train Loss: 0.194
	 Val. Loss: 0.212

Valid loss improved from 0.1570 to 0.1525. Saving checkpoint: weights/checkpoint.pth
Epoch: 28 | Epoch Time: 0m 24s
	Train Loss: 0.182
	 Val. Loss: 0.152

Epoch: 29 | Epoch Time: 0m 24s
	Train Loss: 0.160
	 Val. Loss: 0.154

Valid loss improved from 0.1525 to 0.1524. Saving checkpoint: weights/checkpoint.pth
Epoch: 30 | Epoch Time: 0m 24s
	Train Loss: 0.172
	 Val. Loss: 0.152

Valid loss improved from 0.1524 to 0.1501. Saving checkpoint: weights/checkpoint.pth
Epoch: 31 | Epoch Time: 0m 24s
	Train Loss: 0.157
	 Val. Loss: 0.150

Epoch: 32 | Epoch Time: 0m 24s
	Train Loss: 0.155
	 Val. Loss: 0.152

Epoch: 33 | Epoch Time: 0m 24s
	Train Loss: 0.154
	 Val. Loss: 0.154

Epoch: 34 | Epoch Time: 0m 24s
	Train Loss: 0.154
	 Val. Loss: 0.158

Epoch: 35 | Epoch Time: 0m 24s
	Train Loss: 0.173
	 Val. Loss: 0.185

Valid loss improved from 0.1501 to 0.1464. Saving checkpoint: weights/checkpoint.pth
Epoch: 36 | Epoch Time: 0m 24s
	Train Loss: 0.156
	 Val. Loss: 0.146

Epoch: 37 | Epoch Time: 0m 24s
	Train Loss: 0.150
	 Val. Loss: 0.150

Epoch: 38 | Epoch Time: 0m 24s
	Train Loss: 0.161
	 Val. Loss: 0.157

Epoch: 39 | Epoch Time: 0m 24s
	Train Loss: 0.159
	 Val. Loss: 0.156

Epoch: 40 | Epoch Time: 0m 24s
	Train Loss: 0.162
	 Val. Loss: 0.148

Valid loss improved from 0.1464 to 0.1415. Saving checkpoint: weights/checkpoint.pth
Epoch: 41 | Epoch Time: 0m 24s
	Train Loss: 0.148
	 Val. Loss: 0.141

Epoch: 42 | Epoch Time: 0m 24s
	Train Loss: 0.148
	 Val. Loss: 0.145

Epoch: 43 | Epoch Time: 0m 24s
	Train Loss: 0.147
	 Val. Loss: 0.144

Epoch: 44 | Epoch Time: 0m 24s
	Train Loss: 0.149
	 Val. Loss: 0.154

Epoch: 45 | Epoch Time: 0m 24s
	Train Loss: 0.151
	 Val. Loss: 0.147

Epoch: 46 | Epoch Time: 0m 24s
	Train Loss: 0.159
	 Val. Loss: 0.148

Valid loss improved from 0.1415 to 0.1411. Saving checkpoint: weights/checkpoint.pth
Epoch: 47 | Epoch Time: 0m 24s
	Train Loss: 0.147
	 Val. Loss: 0.141

Epoch: 48 | Epoch Time: 0m 24s
	Train Loss: 0.144
	 Val. Loss: 0.143

Epoch: 49 | Epoch Time: 0m 24s
	Train Loss: 0.145
	 Val. Loss: 0.144

Epoch: 50 | Epoch Time: 0m 24s
	Train Loss: 0.181
	 Val. Loss: 0.148