Please append to this file the current accuracy you have for a given model (Unet, CNN, Xception... ? ) with the number of epochs / hyperparams
And especially the loss at after each epoch -> To be recorded here after each training period

Essential for the report to come !


30 epochs, U-net -> 0.895 F1 score 0.933, training dataset with 1000 samples (from sratch weights)


60 epochs, U-net -> 0.895 F1 score 0.945, training dataset with 1100 samples (antoine, only rotations / flipping but with repeting dataset) (from scratch weights)


80 epochs, Basic U-net -> 0.875, F1 score:0.934, Marin's data augmentation (900 samples):
losses=[0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418, 0.1534139349228806, 0.15346083755294482, 0.15274518079227872, 0.14909644103712505, 0.1469741752743721, 0.148070676939355, 0.16489513190256225, 0.1486604721016354, 0.14407496187422011, 0.14418292934695878, 0.14439637523558405, 0.17220824946959815, 0.14562142946653897, 0.1422411410841677, 0.14169667661190033, 0.16542590166131654, 0.14384175742665928, 0.14193710757626427, 0.14081118105186358, 0.1414958649377028, 0.14115476056933404, 0.14301788061857224, 0.14264949204193222, 0.14213492467999458, 0.1540983316467868, 0.1432336950302124, 0.13879902644289865, 0.13845295078224606, 0.13918791299064953, 0.14067982910407914, 0.1399626374575827, 0.13998906296160485, 0.1409540914495786, 0.14431852747996649, 0.14450248209966554, 0.1390339373714394, 0.15481124737196497, 0.138491816404793, 0.13514803055259916, 0.13461955037381915]

same but only 40 epoch - 0.885, F1=0.940: 
losses = [0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418]

same with 55 epoch - acc=0.889 F1=0.941:
losses=[0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418, 0.1534139349228806, 0.15346083755294482, 0.15274518079227872, 0.14909644103712505, 0.1469741752743721, 0.148070676939355, 0.16489513190256225, 0.1486604721016354, 0.14407496187422011, 0.14418292934695878, 0.14439637523558405, 0.17220824946959815, 0.14562142946653897, 0.1422411410841677, 0.14169667661190033]

