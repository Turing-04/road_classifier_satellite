Please append to this file the current accuracy you have for a given model (Unet, CNN, Xception... ? ) with the number of epochs / hyperparams
And especially the loss at after each epoch -> To be recorded here after each training period

Essential for the report to come !


30 epochs, U-net -> 0.895 F1 score 0.933, training dataset with 1000 samples (from sratch weights)


60 epochs, U-net -> 0.895 F1 score 0.945, training dataset with 1100 samples (antoine, only rotations / flipping but with repeting dataset) (from scratch weights)


80 epochs, Basic U-net -> 0.875, F1 score:0.934, Marin's data augmentation (900 samples):
losses=[0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418, 0.1534139349228806, 0.15346083755294482, 0.15274518079227872, 0.14909644103712505, 0.1469741752743721, 0.148070676939355, 0.16489513190256225, 0.1486604721016354, 0.14407496187422011, 0.14418292934695878, 0.14439637523558405, 0.17220824946959815, 0.14562142946653897, 0.1422411410841677, 0.14169667661190033, 0.16542590166131654, 0.14384175742665928, 0.14193710757626427, 0.14081118105186358, 0.1414958649377028, 0.14115476056933404, 0.14301788061857224, 0.14264949204193222, 0.14213492467999458, 0.1540983316467868, 0.1432336950302124, 0.13879902644289865, 0.13845295078224606, 0.13918791299064953, 0.14067982910407914, 0.1399626374575827, 0.13998906296160485, 0.1409540914495786, 0.14431852747996649, 0.14450248209966554, 0.1390339373714394, 0.15481124737196497, 0.138491816404793, 0.13514803055259916, 0.13461955037381915]

same but only 40 epoch - 0.885, F1=0.940: 
losses = [0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418]

same with 55 epoch - acc=0.889 F1=0.941:
losses=[0.8216245493623946, 0.5422054453028573, 0.45624896215067967, 0.39450105809503133, 0.3740843512614568, 0.33934757073720295, 0.31171434415711297, 0.29711321079068714, 0.28080149047904546, 0.26728939218653575, 0.26565327213870155, 0.2517532204588254, 0.2347977066040039, 0.2290028390950627, 0.21724024484554927, 0.23011629068189196, 0.2221700151099099, 0.2031264789071348, 0.1955354364381896, 0.18760695492227872, 0.21182107168767186, 0.1983269962337282, 0.17863923407263227, 0.1731806512673696, 0.17028251164489322, 0.1862174954182572, 0.17434434306290414, 0.16803391239709325, 0.17140161726209852, 0.1621779717836115, 0.15935334119531844, 0.16157615663276778, 0.18765870604250165, 0.1557986015578111, 0.1586538788676262, 0.15253067567944525, 0.16426936250593926, 0.17222825088434748, 0.15194130311409632, 0.14814110787378418, 0.1534139349228806, 0.15346083755294482, 0.15274518079227872, 0.14909644103712505, 0.1469741752743721, 0.148070676939355, 0.16489513190256225, 0.1486604721016354, 0.14407496187422011, 0.14418292934695878, 0.14439637523558405, 0.17220824946959815, 0.14562142946653897, 0.1422411410841677, 0.14169667661190033]

50 epochs, cnn16, F1:0.560,	acc:0.781
[1.4612571475240919, 1.3821242952346802, 1.3276965747939216, 1.278738153245714, 1.2363285130924648, 1.2042076455222235, 1.1807094981935289, 1.1563590003384485, 1.1370153286721971, 1.1222055651081932, 1.1078563059700859, 1.0966064258416495, 1.088292614751392, 1.0806414274374645, 1.0752597469753689, 1.0678815428415935, 1.0633759187327492, 1.0595504997836218, 1.058351985613505, 1.0532632244957818, 1.0512659102016024, 1.0518066730764177, 1.0493853500154284, 1.0458776108423868, 1.0410099233521355, 1.0451514707671272, 1.0445100017388662, 1.0404670935206943, 1.0436071097850799, 1.0361200388272602, 1.0333451669745974, 1.0404689719941882, 1.0316878305541144, 1.0356717885865105, 1.0329800502459208, 1.031337748368581, 1.0324597701761458, 1.0280844516224332, 1.0336266794469622, 1.0294600987434388, 1.029387230740653, 1.0293171841568416, 1.0296000085936652, 1.0273783842722575, 1.0276922982268863, 1.024765402475993, 1.0228810180558099, 1.025966581636005, 1.0279537264506022, 1.0236940484576755]

50 epochs, cnn4, no submission yet
[1.3029563607109917, 1.2517039288414848, 1.21849035554462, 1.194526960849762, 1.1709564418262906, 1.1538910694917044, 1.1273357105255126, 1.0946927369965447, 1.0747257607513003, 1.0631217186980777, 1.050767591661877, 1.046948635313246, 1.0409223857190875, 1.036800389819675, 1.0363096311357287, 1.030641153521008, 1.0288906863000657, 1.0258088764879438, 1.0260296408335368, 1.0237021402517954, 1.025619367758433, 1.0226485231187608, 1.0194274469216664, 1.0191140210628509, 1.0204369299941594, 1.0178564993540447, 1.0141026037269167, 1.0153743267059325, 1.0119534611701966, 1.0116905433601804, 1.0088933424154918, 1.0094050199455684, 1.0118589198589325, 1.0069289984967973, 1.0142938519848717, 1.00818874835968, 1.0100191685888502, 1.0124075763755374, 1.0090126225683425, 1.006687547100915, 1.0103006439738804, 1.0066130636798012, 1.0065248771508535, 1.0061466281943852, 1.004367254310184, 1.005252998669942, 1.005622718334198, 1.0039466410213047, 1.0063876582516564, 1.0049560157457988]